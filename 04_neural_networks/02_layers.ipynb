{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4.2: Common Neural Network Layers\n",
    "\n",
    "Learn about the building blocks of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Layer (nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear transformation: y = x @ W.T + b\n",
    "linear = nn.Linear(in_features=4, out_features=3)\n",
    "\n",
    "print(f\"Linear(4, 3)\")\n",
    "print(f\"  Weight shape: {linear.weight.shape}\")  # (out, in)\n",
    "print(f\"  Bias shape: {linear.bias.shape}\")      # (out,)\n",
    "\n",
    "x = torch.randn(2, 4)  # Batch of 2, each with 4 features\n",
    "output = linear(x)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without bias\n",
    "linear_no_bias = nn.Linear(4, 3, bias=False)\n",
    "print(f\"Linear without bias - has bias: {linear_no_bias.bias is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "print(f\"Input x: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU: max(0, x) - Most common!\n",
    "relu = nn.ReLU()\n",
    "print(f\"ReLU(x): {relu(x)}\")\n",
    "print(\"  Negative values become 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid: 1/(1+e^-x) - Output between 0 and 1\n",
    "sigmoid = nn.Sigmoid()\n",
    "print(f\"Sigmoid(x): {sigmoid(x)}\")\n",
    "print(\"  Used for binary classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tanh: Output between -1 and 1\n",
    "tanh = nn.Tanh()\n",
    "print(f\"Tanh(x): {tanh(x)}\")\n",
    "print(\"  Like sigmoid but centered at 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax: Converts to probability distribution\n",
    "softmax = nn.Softmax(dim=0)\n",
    "logits = torch.tensor([2.0, 1.0, 0.1])\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Softmax(logits): {softmax(logits)}\")\n",
    "print(f\"Sum: {softmax(logits).sum()}\")  # Always sums to 1\n",
    "print(\"  Used for multi-class classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeakyReLU: Allows small negative values\n",
    "leaky_relu = nn.LeakyReLU(negative_slope=0.1)\n",
    "print(f\"LeakyReLU(x): {leaky_relu(x)}\")\n",
    "print(\"  Allows small gradient for negative values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use which activation:\n",
    "| Activation | Use Case |\n",
    "|------------|----------|\n",
    "| ReLU | Default choice for hidden layers |\n",
    "| Sigmoid | Binary classification output |\n",
    "| Softmax | Multi-class classification output |\n",
    "| Tanh | When you need output in [-1, 1] |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(p=0.5)  # 50% of neurons dropped\n",
    "\n",
    "x = torch.ones(10)\n",
    "print(f\"Input: {x}\")\n",
    "\n",
    "# In training mode - dropout active\n",
    "dropout.train()\n",
    "print(f\"Training mode output: {dropout(x)}\")\n",
    "print(\"  Some values are 0, others are scaled up\")\n",
    "\n",
    "# In eval mode - dropout disabled\n",
    "dropout.eval()\n",
    "print(f\"Eval mode output: {dropout(x)}\")\n",
    "print(\"  All values pass through\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout Purpose:**\n",
    "- Prevents overfitting\n",
    "- Randomly \"drops\" neurons during training\n",
    "- Forces network to be more robust\n",
    "- **ALWAYS disable during evaluation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm normalizes across the batch\n",
    "batch_norm = nn.BatchNorm1d(num_features=3)\n",
    "\n",
    "# Input: (batch_size, features)\n",
    "x = torch.randn(4, 3) * 10 + 5  # Mean ~5, std ~10\n",
    "print(f\"Input (mean, std): {x.mean():.2f}, {x.std():.2f}\")\n",
    "\n",
    "batch_norm.train()\n",
    "output = batch_norm(x)\n",
    "print(f\"After BatchNorm: mean={output.mean():.4f}, std={output.std():.4f}\")\n",
    "print(\"  Normalizes to approximately mean=0, std=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Normalization Purpose:**\n",
    "- Stabilizes training\n",
    "- Allows higher learning rates\n",
    "- Reduces internal covariate shift\n",
    "- Has learnable scale and shift parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts integer indices to dense vectors\n",
    "# Great for words, categories, etc.\n",
    "embedding = nn.Embedding(num_embeddings=10, embedding_dim=4)\n",
    "\n",
    "# Input: indices (like word IDs)\n",
    "indices = torch.tensor([1, 2, 5, 0])\n",
    "print(f\"Input indices: {indices}\")\n",
    "\n",
    "vectors = embedding(indices)\n",
    "print(f\"Output vectors shape: {vectors.shape}\")\n",
    "print(f\"Each index becomes a {embedding.embedding_dim}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding Purpose:**\n",
    "- Convert categorical data to dense vectors\n",
    "- Used for words, user IDs, product IDs, etc.\n",
    "- Vectors are learned during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of writing a class, use Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(20, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2)\n",
    ")\n",
    "\n",
    "print(\"Model structure:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 10)  # Batch of 5\n",
    "model.eval()  # Disable dropout for consistent output\n",
    "output = model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Access specific layers\n",
    "print(f\"\\nFirst layer: {model[0]}\")\n",
    "print(f\"First layer weight shape: {model[0].weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Common layer patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Patterns:\n",
    "\n",
    "**Classification Network:**\n",
    "```\n",
    "Linear -> ReLU -> Dropout -> Linear -> ReLU -> Linear -> Softmax\n",
    "```\n",
    "\n",
    "**Regression Network:**\n",
    "```\n",
    "Linear -> ReLU -> Linear -> ReLU -> Linear (no activation at end)\n",
    "```\n",
    "\n",
    "**With Batch Norm:**\n",
    "```\n",
    "Linear -> BatchNorm -> ReLU -> Dropout -> ...\n",
    "```\n",
    "\n",
    "**Typical Hidden Layer:**\n",
    "```\n",
    "Linear -> BatchNorm -> ReLU -> Dropout\n",
    "```\n",
    "(Order can vary, this is one common pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example classification network\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(784, 256),    # Input layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(256, 128),    # Hidden layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(128, 10),     # Output: 10 classes\n",
    "    # Note: Often Softmax is in the loss function, not here\n",
    ")\n",
    "\n",
    "print(\"Example classifier for MNIST:\")\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Essential Layers\n",
    "\n",
    "| Layer | Purpose |\n",
    "|-------|--------|\n",
    "| `nn.Linear(in, out)` | Fully connected layer |\n",
    "| `nn.ReLU()` | Activation (most common) |\n",
    "| `nn.Sigmoid()` | Activation (binary output) |\n",
    "| `nn.Softmax(dim)` | Activation (multi-class) |\n",
    "| `nn.Dropout(p)` | Regularization |\n",
    "| `nn.BatchNorm1d(n)` | Normalization |\n",
    "| `nn.Embedding(n, dim)` | For categorical data |\n",
    "\n",
    "### Building Networks\n",
    "- Use `nn.Sequential` for simple architectures\n",
    "- Use custom `nn.Module` for complex ones\n",
    "- Remember: `model.train()` and `model.eval()`\n",
    "\n",
    "### Tips\n",
    "- Start with ReLU, it usually works well\n",
    "- Add Dropout to prevent overfitting\n",
    "- BatchNorm can speed up training\n",
    "- Check layer shapes match: out of one = in of next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** Open `03_building_networks.ipynb` for complete examples!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
