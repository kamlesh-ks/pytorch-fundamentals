{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4.3: Building Complete Networks\n",
    "\n",
    "Put it all together to build practical neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Classifier (e.g., for MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feedforward network for classification.\n",
    "    Input: Flattened image (e.g., 28x28 = 784 pixels)\n",
    "    Output: Class probabilities (e.g., 10 digits)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden_sizes=[256, 128], num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build layers dynamically\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        \n",
    "        # Combine into sequential\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Flatten input if needed (for images)\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test\n",
    "model = SimpleClassifier()\n",
    "print(model)\n",
    "\n",
    "# Test with fake batch of images\n",
    "fake_images = torch.randn(32, 1, 28, 28)  # 32 grayscale 28x28 images\n",
    "output = model(fake_images)\n",
    "print(f\"\\nInput shape: {fake_images.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output (logits for first image): {output[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regression Network (e.g., House Prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Network for predicting continuous values.\n",
    "    No activation on output layer!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_features, hidden_sizes=[64, 32]):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_features\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output: single value, NO activation!\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze(-1)  # Remove last dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "model = RegressionNetwork(input_features=13)  # 13 house features\n",
    "print(model)\n",
    "\n",
    "fake_house_data = torch.randn(16, 13)  # 16 houses, 13 features each\n",
    "predicted_prices = model(fake_house_data)\n",
    "print(f\"\\nInput shape: {fake_house_data.shape}\")\n",
    "print(f\"Output shape: {predicted_prices.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Network with Skip Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block: output = ReLU(x + F(x))\n",
    "    Skip connections help with training deep networks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(features, features),\n",
    "            nn.BatchNorm1d(features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features, features),\n",
    "            nn.BatchNorm1d(features)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Skip connection: add input to output\n",
    "        return self.relu(x + self.block(x))\n",
    "\n",
    "\n",
    "class ResidualNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_blocks, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial projection\n",
    "        self.input_layer = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Stack of residual blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[ResidualBlock(hidden_size) for _ in range(num_blocks)]\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResidualNetwork(784, 256, num_blocks=3, num_classes=10)\n",
    "print(f\"Residual Network with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Test\n",
    "x = torch.randn(8, 784)\n",
    "output = model(x)\n",
    "print(f\"Input: {x.shape} -> Output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Custom Forward Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Network with multiple output heads.\n",
    "    Useful for multi-task learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Separate heads for different tasks\n",
    "        self.classification_head = nn.Linear(64, 10)  # 10 classes\n",
    "        self.regression_head = nn.Linear(64, 1)       # 1 value\n",
    "    \n",
    "    def forward(self, x, task='both'):\n",
    "        # Shared processing\n",
    "        shared_features = self.shared(x)\n",
    "        \n",
    "        if task == 'classification':\n",
    "            return self.classification_head(shared_features)\n",
    "        elif task == 'regression':\n",
    "            return self.regression_head(shared_features)\n",
    "        else:  # 'both'\n",
    "            return {\n",
    "                'classification': self.classification_head(shared_features),\n",
    "                'regression': self.regression_head(shared_features)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiHeadNetwork(784)\n",
    "x = torch.randn(4, 784)\n",
    "\n",
    "# Different ways to use it\n",
    "class_output = model(x, task='classification')\n",
    "reg_output = model(x, task='regression')\n",
    "both_output = model(x, task='both')\n",
    "\n",
    "print(f\"Classification output: {class_output.shape}\")\n",
    "print(f\"Regression output: {reg_output.shape}\")\n",
    "print(f\"Both outputs: classification={both_output['classification'].shape}, \"\n",
    "      f\"regression={both_output['regression'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Saving and Loading Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Models\n",
    "\n",
    "```python\n",
    "# Save weights only (recommended)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "# Load weights\n",
    "model = SimpleClassifier()  # Create architecture first\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Save with optimizer state (for resuming training)\n",
    "checkpoint = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': epoch\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Summary Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model):\n",
    "    \"\"\"Print a summary of the model.\"\"\"\n",
    "    print(f\"{'Layer':<30} {'Output Shape':<20} {'Params':>10}\")\n",
    "    print(\"-\" * 62)\n",
    "    \n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        params = param.numel()\n",
    "        total_params += params\n",
    "        print(f\"{name:<30} {str(param.shape):<20} {params:>10,}\")\n",
    "    \n",
    "    print(\"-\" * 62)\n",
    "    print(f\"{'Total Parameters:':<50} {total_params:>10,}\")\n",
    "    print(f\"{'Trainable Parameters:':<50} \"\n",
    "          f\"{sum(p.numel() for p in model.parameters() if p.requires_grad):>10,}\")\n",
    "\n",
    "model = SimpleClassifier(784, [256, 128], 10)\n",
    "model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Building Networks\n",
    "\n",
    "1. **CLASSIFIER**: End with `Linear(hidden, num_classes)`\n",
    "   - Softmax usually in loss function (`CrossEntropyLoss`)\n",
    "\n",
    "2. **REGRESSION**: End with `Linear(hidden, 1)` - NO activation!\n",
    "   - Use `MSELoss` or `L1Loss`\n",
    "\n",
    "3. **SKIP CONNECTIONS**: `output = x + F(x)`\n",
    "   - Help train deeper networks\n",
    "\n",
    "4. **MULTI-TASK**: Multiple output heads sharing features\n",
    "\n",
    "### Architecture Tips\n",
    "\n",
    "- Start simple, add complexity as needed\n",
    "- Power of 2 for hidden sizes (64, 128, 256, 512)\n",
    "- Add Dropout between layers to prevent overfitting\n",
    "- BatchNorm can stabilize training\n",
    "- Match input/output sizes of consecutive layers!\n",
    "\n",
    "### Debugging\n",
    "\n",
    "- Print shapes at each step\n",
    "- Start with tiny data to test\n",
    "- Check gradients are flowing\n",
    "- Verify output shape matches labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** Move to Module 5 to learn how to train them!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
