{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3.3: Practical Gradient Example\n",
    "\n",
    "Let's use gradients to solve a real problem: **linear regression!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "We have data points that follow: `y = 2x + 1` (with some noise)\n",
    "\n",
    "We want to **LEARN** the parameters (`w=2`, `b=1`) from the data.\n",
    "\n",
    "This is the simplest machine learning problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data: y = 2x + 1 + noise\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "X = torch.linspace(0, 10, 20)  # 20 points from 0 to 10\n",
    "y_true = 2 * X + 1 + torch.randn(20) * 0.5  # True relationship + noise\n",
    "\n",
    "print(f\"X (first 5): {X[:5]}\")\n",
    "print(f\"y (first 5): {y_true[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with random guesses\n",
    "w = torch.tensor(0.0, requires_grad=True)  # Weight (should learn ~2)\n",
    "b = torch.tensor(0.0, requires_grad=True)  # Bias (should learn ~1)\n",
    "\n",
    "print(f\"Initial w = {w.item():.4f} (should be ~2)\")\n",
    "print(f\"Initial b = {b.item():.4f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "\n",
    "print(\"Epoch | Loss     | w      | b\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # 1. FORWARD PASS: Make predictions\n",
    "    y_pred = w * X + b\n",
    "    \n",
    "    # 2. COMPUTE LOSS: Mean Squared Error\n",
    "    loss = ((y_pred - y_true) ** 2).mean()\n",
    "    \n",
    "    # 3. BACKWARD PASS: Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # 4. UPDATE PARAMETERS (gradient descent)\n",
    "    # Note: We use torch.no_grad() because we don't want to track\n",
    "    # the update operation in the computational graph\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # 5. ZERO GRADIENTS for next iteration\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"{epoch:5d} | {loss.item():.4f} | {w.item():.4f} | {b.item():.4f}\")\n",
    "\n",
    "print(f\"\\n Final: w = {w.item():.4f}, b = {b.item():.4f}\")\n",
    "print(f\" True:  w = 2.0000, b = 1.0000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Happened?\n",
    "\n",
    "Each iteration:\n",
    "\n",
    "1. **FORWARD**: Predicted `y = w*x + b`\n",
    "\n",
    "2. **LOSS**: Measured error with `MSE = mean((predicted - actual)Â²)`\n",
    "\n",
    "3. **BACKWARD**: PyTorch computed gradients:\n",
    "   - `d(loss)/dw`: How much w affects the loss\n",
    "   - `d(loss)/db`: How much b affects the loss\n",
    "\n",
    "4. **UPDATE**: Moved parameters in opposite direction of gradient\n",
    "   - `w = w - learning_rate * gradient`\n",
    "   - If gradient is positive, w decreases\n",
    "   - If gradient is negative, w increases\n",
    "   - This REDUCES the loss!\n",
    "\n",
    "5. **ZERO GRAD**: Cleared gradients for next iteration\n",
    "   (Remember: gradients accumulate by default)\n",
    "\n",
    "**This is GRADIENT DESCENT - the foundation of all deep learning!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Intuition\n",
    "\n",
    "Imagine you're blindfolded on a hilly terrain.\n",
    "You want to reach the lowest point (minimum loss).\n",
    "\n",
    "The **GRADIENT** tells you:\n",
    "- Which direction is uphill\n",
    "- How steep the slope is\n",
    "\n",
    "To go **DOWNHILL**, you step in the **OPPOSITE** direction:\n",
    "```\n",
    "new_position = old_position - step_size * gradient\n",
    "```\n",
    "\n",
    "The **LEARNING RATE** controls step size:\n",
    "- Too small: Slow progress\n",
    "- Too large: Might overshoot the minimum\n",
    "- Just right: Efficient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Real PyTorch Code\n",
    "\n",
    "In real PyTorch code, you'll use:\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create optimizer (handles updates for you)\n",
    "optimizer = optim.SGD([w, b], lr=0.01)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y_true)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Clear old gradients\n",
    "    loss.backward()        # Compute new gradients\n",
    "    optimizer.step()       # Update parameters\n",
    "```\n",
    "\n",
    "The optimizer does steps 4-5 automatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Training Loop Pattern\n",
    "\n",
    "```python\n",
    "1. Forward pass:  y_pred = model(x)\n",
    "2. Compute loss:  loss = loss_function(y_pred, y_true)\n",
    "3. Zero grads:    optimizer.zero_grad()\n",
    "4. Backward pass: loss.backward()\n",
    "5. Update params: optimizer.step()\n",
    "```\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- Gradients point toward **INCREASING** loss\n",
    "- We move **OPPOSITE** direction to **DECREASE** loss\n",
    "- Learning rate controls step size\n",
    "- This is **GRADIENT DESCENT**\n",
    "\n",
    "**YOU NOW UNDERSTAND HOW NEURAL NETWORKS LEARN!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Congratulations!** You've completed Module 3!\n",
    "\n",
    "**Next:** Move to Module 4 to build actual neural networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
