{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3.1: Gradient Basics\n",
    "\n",
    "Understand what gradients are and how PyTorch computes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a gradient?\n",
    "\n",
    "A gradient (derivative) tells us: **\"How much does the OUTPUT change when I change the INPUT?\"**\n",
    "\n",
    "**Example:** `y = x²`\n",
    "- When x = 3, y = 9\n",
    "- Gradient dy/dx = 2x = 6\n",
    "- This means: if x increases by 1, y increases by about 6\n",
    "\n",
    "### Why this matters for Neural Networks:\n",
    "- We want to minimize error (loss)\n",
    "- Gradients tell us which direction to adjust weights\n",
    "- We move weights in the direction that reduces error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tracking gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular tensor - no gradient tracking\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"Regular tensor: {x}\")\n",
    "print(f\"requires_grad: {x.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor WITH gradient tracking\n",
    "x_grad = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(f\"Tensor with gradients: {x_grad}\")\n",
    "print(f\"requires_grad: {x_grad.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or enable it later\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "x.requires_grad = True\n",
    "print(f\"Enabled later: requires_grad = {x.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computing gradients with backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: y = x²\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = x ** 2  # y = x²\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = x² = {y}\")\n",
    "\n",
    "# Compute gradient\n",
    "y.backward()  # Computes dy/dx\n",
    "\n",
    "print(f\"\\nGradient dy/dx at x=3: {x.grad}\")\n",
    "print(\"(dy/dx = 2x = 2*3 = 6 ✓)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradients with vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y.sum()  # Need a scalar for backward()\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = x² = {y}\")\n",
    "print(f\"z = sum(y) = {z}\")\n",
    "\n",
    "z.backward()\n",
    "print(f\"\\nGradient dz/dx: {x.grad}\")\n",
    "print(\"(dz/dx = 2x = [2, 4, 6] ✓)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. More complex example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# y = w*x + b (like a simple neural network!)\n",
    "y = w * x + b\n",
    "\n",
    "print(f\"x = {x.item()}\")\n",
    "print(f\"w = {w.item()}\")\n",
    "print(f\"b = {b.item()}\")\n",
    "print(f\"y = w*x + b = {y.item()}\")\n",
    "\n",
    "# Backward pass\n",
    "y.backward()\n",
    "\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"dy/dx = {x.grad}\")  # Should be w = 3\n",
    "print(f\"dy/dw = {w.grad}\")  # Should be x = 2\n",
    "print(f\"dy/db = {b.grad}\")  # Should be 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why these gradients?**\n",
    "```\n",
    "y = w*x + b\n",
    "dy/dw = x = 2  (w's effect on y)\n",
    "dy/dx = w = 3  (x's effect on y)\n",
    "dy/db = 1      (b's direct effect on y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradient accumulation (important!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# First computation\n",
    "y1 = x * 3\n",
    "y1.backward()\n",
    "print(f\"After first backward: x.grad = {x.grad}\")\n",
    "\n",
    "# Second computation - gradients ACCUMULATE!\n",
    "y2 = x * 3\n",
    "y2.backward()\n",
    "print(f\"After second backward: x.grad = {x.grad}\")  # 6, not 3!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING:** Gradients accumulate by default!\n",
    "\n",
    "This is useful for some algorithms, but usually you want:\n",
    "```python\n",
    "x.grad.zero_()  # Reset gradients before each backward\n",
    "```\n",
    "\n",
    "In training loops, use:\n",
    "```python\n",
    "optimizer.zero_grad()  # Resets all parameter gradients\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detaching tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x * 2\n",
    "\n",
    "# Detach: create a tensor that doesn't track gradients\n",
    "y_detached = y.detach()\n",
    "print(f\"y requires_grad: {y.requires_grad}\")\n",
    "print(f\"y_detached requires_grad: {y_detached.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.no_grad(): disable gradient tracking in a block\n",
    "with torch.no_grad():\n",
    "    z = x * 2\n",
    "    print(f\"Inside no_grad, z requires_grad: {z.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to use no_grad():**\n",
    "- During inference (prediction), not training\n",
    "- Saves memory and computation\n",
    "- Use for validation/test phases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **GRADIENTS** tell us how to adjust parameters\n",
    "   - They point toward increasing values\n",
    "   - We go OPPOSITE direction to minimize loss\n",
    "\n",
    "2. **requires_grad=True** tracks operations\n",
    "   - Creates a computational graph\n",
    "   - Enables gradient computation\n",
    "\n",
    "3. **.backward()** computes gradients\n",
    "   - Fills in the `.grad` attribute\n",
    "   - Must be called on a scalar\n",
    "\n",
    "4. **GRADIENTS ACCUMULATE**\n",
    "   - Zero them with `.zero_grad()` before each step\n",
    "   - Or use `optimizer.zero_grad()`\n",
    "\n",
    "5. **no_grad()** disables tracking\n",
    "   - Use during inference\n",
    "   - Saves memory\n",
    "\n",
    "### The Learning Process\n",
    "```\n",
    "Forward:  input → prediction → loss\n",
    "Backward: loss → gradients → update weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** Open `02_computational_graph.ipynb` to learn about computational graphs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
