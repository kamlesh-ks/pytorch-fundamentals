{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3.2: Computational Graph\n",
    "\n",
    "Understand how PyTorch tracks operations to compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is a computational graph?\n",
    "\n",
    "PyTorch builds a graph of all operations performed on tensors with `requires_grad=True`. This graph is used to compute gradients.\n",
    "\n",
    "**Example:** `y = (a + b) * c`\n",
    "\n",
    "```\n",
    "       [y]          <- Output\n",
    "        |\n",
    "       [*]          <- Multiplication\n",
    "      /   \\\n",
    "   [+]     [c]      <- Addition and c\n",
    "  /   \\\n",
    "[a]   [b]           <- Inputs\n",
    "```\n",
    "\n",
    "PyTorch records this graph and uses it to compute gradients via the chain rule of calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Seeing the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "c = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# Build computation\n",
    "temp = a + b\n",
    "y = temp * c\n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"c = {c}\")\n",
    "print(f\"temp = a + b = {temp}\")\n",
    "print(f\"y = temp * c = {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each tensor knows its history\n",
    "print(f\"y.grad_fn: {y.grad_fn}\")  # Multiplication\n",
    "print(f\"temp.grad_fn: {temp.grad_fn}\")  # Addition\n",
    "\n",
    "# Leaf tensors have no grad_fn\n",
    "print(f\"\\na.grad_fn: {a.grad_fn}\")  # None (leaf)\n",
    "print(f\"a.is_leaf: {a.is_leaf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chain rule in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain rule says:\n",
    "```\n",
    "dy/da = (dy/d_temp) * (d_temp/da)\n",
    "```\n",
    "\n",
    "For `y = (a + b) * c`:\n",
    "```\n",
    "temp = a + b\n",
    "y = temp * c\n",
    "\n",
    "dy/d_temp = c = 4\n",
    "d_temp/da = 1\n",
    "\n",
    "dy/da = c * 1 = 4\n",
    "dy/db = c * 1 = 4\n",
    "dy/dc = temp = 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "\n",
    "print(f\"dy/da = {a.grad} (expected: c = 4)\")\n",
    "print(f\"dy/db = {b.grad} (expected: c = 4)\")\n",
    "print(f\"dy/dc = {c.grad} (expected: a+b = 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dynamic computation graphs (PyTorch's superpower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses **DYNAMIC** computational graphs:\n",
    "- Graph is built fresh for each forward pass\n",
    "- Can change based on input data\n",
    "- Allows for loops, conditionals, etc.\n",
    "\n",
    "This is different from TensorFlow 1.x (static graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_computation(x, do_square=True):\n",
    "    \"\"\"Graph changes based on the flag!\"\"\"\n",
    "    if do_square:\n",
    "        return x ** 2\n",
    "    else:\n",
    "        return x * 3\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# First call - squaring\n",
    "y1 = dynamic_computation(x, do_square=True)\n",
    "y1.backward()\n",
    "print(f\"With squaring: dy/dx = {x.grad}\")  # 2*2 = 4\n",
    "\n",
    "# Reset gradient\n",
    "x.grad.zero_()\n",
    "\n",
    "# Second call - tripling\n",
    "y2 = dynamic_computation(x, do_square=False)\n",
    "y2.backward()\n",
    "print(f\"With tripling: dy/dx = {x.grad}\")  # 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Graph retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "print(f\"Before backward: y.grad_fn = {y.grad_fn}\")\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(f\"After backward: Gradients computed\")\n",
    "print(f\"x.grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** By default, the graph is freed after `backward()`. \n",
    "\n",
    "To keep it (rare), use: `y.backward(retain_graph=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. How this applies to neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating a tiny neural network\n",
    "# Input -> Linear -> ReLU -> Output\n",
    "\n",
    "# \"Weights\" (learnable parameters)\n",
    "w1 = torch.tensor([[0.1, 0.2], [0.3, 0.4]], requires_grad=True)\n",
    "b1 = torch.tensor([0.1, 0.1], requires_grad=True)\n",
    "\n",
    "# Input (not learnable)\n",
    "x = torch.tensor([1.0, 2.0])\n",
    "\n",
    "# Forward pass\n",
    "linear_out = x @ w1 + b1  # Linear layer\n",
    "relu_out = torch.relu(linear_out)  # ReLU activation\n",
    "loss = relu_out.sum()  # Simple \"loss\"\n",
    "\n",
    "print(f\"Input x: {x}\")\n",
    "print(f\"After linear: {linear_out}\")\n",
    "print(f\"After ReLU: {relu_out}\")\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Gradients computed!\")\n",
    "print(f\"w1.grad:\\n{w1.grad}\")\n",
    "print(f\"b1.grad: {b1.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **EXACTLY** what happens in neural network training:\n",
    "1. **Forward**: data flows through layers\n",
    "2. **Loss**: compute how wrong we are\n",
    "3. **Backward**: gradients flow back through layers\n",
    "4. **Update**: adjust weights using gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Computational Graph\n",
    "- Records all operations on `requires_grad=True` tensors\n",
    "- Used to compute gradients via chain rule\n",
    "- Built dynamically during forward pass\n",
    "\n",
    "### Key Attributes\n",
    "| Attribute | Description |\n",
    "|-----------|-------------|\n",
    "| `tensor.grad_fn` | Function that created this tensor |\n",
    "| `tensor.is_leaf` | Is this a leaf node (no grad_fn)? |\n",
    "| `tensor.grad` | Computed gradient (after backward) |\n",
    "\n",
    "### Dynamic Graphs\n",
    "- Graph can change each forward pass\n",
    "- Supports Python control flow (if, for, while)\n",
    "- Graph is freed after `backward()` by default\n",
    "\n",
    "### The Flow\n",
    "```\n",
    "Forward:  Build graph, compute output\n",
    "Backward: Traverse graph, compute gradients\n",
    "Update:   Use gradients to update weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Next:** Open `03_practical_example.ipynb` to see gradients in action!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
